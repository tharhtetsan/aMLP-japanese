{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4c7dd5",
   "metadata": {},
   "source": [
    "python run-squad.py --restore_from aMLP-Summalizer-large-ja --pred_dataset summalize-testdata.json --verbose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c9005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f07463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from modeling import model as build_model\n",
    "from modeling import projection\n",
    "from encoder import get_encoder\n",
    "CHECKPOINT_DIR = 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482dd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_from = r\"E:\\CloudSource\\CloudSource_share\\tharhtetsan\\testing\\aMLP-Summalizer-large-ja\"\n",
    "run_name = \"aMLP-Summalizer-large-ja\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b46b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad_json(filename, to_val=False):\n",
    "    with open(filename,encoding=\"utf-8\") as f:\n",
    "        squad = json.loads(f.read())\n",
    "    context, question, answer_start, answer_end, question_id, answer = [], [], [], [], [], []\n",
    "    num_quest = 0\n",
    "    for data in squad[\"data\"]:\n",
    "        for p in data[\"paragraphs\"]:\n",
    "            c = p[\"context\"]\n",
    "            for q in p[\"qas\"]:\n",
    "                if \"is_impossible\" not in q or not q[\"is_impossible\"]:\n",
    "                    for a in (q[\"answers\"][:1] if to_val else q[\"answers\"]):\n",
    "                        answer.append(a[\"text\"])\n",
    "                        context.append(c)\n",
    "                        question.append(q[\"question\"])\n",
    "                        if \"id\" in q:\n",
    "                            question_id.append(q[\"id\"])\n",
    "                        else:\n",
    "                            question_id.append(str(num_quest))\n",
    "                        answer_start.append(a[\"answer_start\"])\n",
    "                        answer_end.append(a[\"answer_start\"]+len(a[\"text\"]))\n",
    "                        num_quest += 1\n",
    "                elif not to_val:\n",
    "                    answer.append(\"\")\n",
    "                    context.append(c)\n",
    "                    question.append(q[\"question\"])\n",
    "                    if \"id\" in q:\n",
    "                        question_id.append(q[\"id\"])\n",
    "                    else:\n",
    "                        question_id.append(str(num_quest))\n",
    "                    answer_start.append(-1)\n",
    "                    answer_end.append(-1)\n",
    "                    num_quest += 1\n",
    "    print(f'read {len(context)} contexts from {filename}.')\n",
    "    return context, question, answer_start, answer_end, question_id, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d5f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def jaccard_wd(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def get_best_indexes(logits, n_best_size):\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8fef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class squad_model(tf.keras.Model):\n",
    "    def __init__(self, model, conf_dict):\n",
    "        super(squad_model, self).__init__(name='squad_model')\n",
    "        self.model = model\n",
    "        self.projection = projection(conf_dict[\"num_hidden\"], 2, name='squad_output')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_ids, input_weights = inputs\n",
    "        lm_output, _ = self.model(inputs=[input_ids, input_weights])\n",
    "        logits = self.projection(lm_output)\n",
    "        logits = tf.transpose(logits, [2, 0, 1])\n",
    "        unstacked_logits = tf.unstack(logits, axis=0)\n",
    "        start_logits, end_logits = unstacked_logits[0], unstacked_logits[1]\n",
    "        return [start_logits, end_logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1237d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(labels, logits):\n",
    "        num_vocabrary = logits.shape.as_list()[-1]\n",
    "        flat_labels = tf.reshape(labels, [-1])\n",
    "        flat_labels = tf.cast(flat_labels, tf.int32)\n",
    "        flat_logits = tf.reshape(logits, [-1, num_vocabrary])\n",
    "        one_hot_labels = tf.one_hot(flat_labels, depth=num_vocabrary, dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(flat_logits)\n",
    "        loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cf510d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = os.path.join(restore_from, \"vocabulary.txt\")\n",
    "hpm_path = os.path.join(restore_from,\"hparams.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13f7903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hpm_path) as f:\n",
    "        conf_dict = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "892cfb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bpe_path,encoding=\"utf-8\") as f:\n",
    "    ww = np.sum([1 if ('##' in l) else 0 for l in f.readlines()]) > 0\n",
    "enc = get_encoder(bpe_path, 'emoji.json', ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b231b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = conf_dict[\"num_vocab\"]\n",
    "EOT_TOKEN = vocab_size - 1\n",
    "MASK_TOKEN = vocab_size - 2\n",
    "SEP_TOKEN = vocab_size - 3\n",
    "CLS_TOKEN = vocab_size - 4\n",
    "\n",
    "batch_size = 4 # default\n",
    "\n",
    "\n",
    "max_seq_length = conf_dict[\"num_ctx\"]\n",
    "max_predictions = 1\n",
    "log_dir = \"\"\n",
    "max_answer_length = 50\n",
    "num_best_indexes = 20\n",
    "\n",
    "\n",
    "\n",
    "pred_dataset = \"summalize-testdata.json\"\n",
    "dataset = \"summalize-testdata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4d1f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on 1 replicas\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(CHECKPOINT_DIR,run_name), exist_ok=True)\n",
    "strategy = tf.distribute.get_strategy()\n",
    "print(f\"Running on {strategy.num_replicas_in_sync} replicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3886e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class squad_model(tf.keras.Model):\n",
    "    def __init__(self, model, conf_dict):\n",
    "        super(squad_model, self).__init__(name='squad_model')\n",
    "        self.model = model\n",
    "        self.projection = projection(conf_dict[\"num_hidden\"], 2, name='squad_output')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_ids, input_weights = inputs\n",
    "        lm_output, _ = self.model(inputs=[input_ids, input_weights])\n",
    "        logits = self.projection(lm_output)\n",
    "        logits = tf.transpose(logits, [2, 0, 1])\n",
    "        unstacked_logits = tf.unstack(logits, axis=0)\n",
    "        start_logits, end_logits = unstacked_logits[0], unstacked_logits[1]\n",
    "        return [start_logits, end_logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c11cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(labels, logits):\n",
    "        num_vocabrary = logits.shape.as_list()[-1]\n",
    "        flat_labels = tf.reshape(labels, [-1])\n",
    "        flat_labels = tf.cast(flat_labels, tf.int32)\n",
    "        flat_logits = tf.reshape(logits, [-1, num_vocabrary])\n",
    "        one_hot_labels = tf.one_hot(flat_labels, depth=num_vocabrary, dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(flat_logits)\n",
    "        loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fe87220",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    counter = 1\n",
    "    lossmodel = tf.keras.models.load_model(restore_from, \\\n",
    "                    custom_objects={'crossentropy': crossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28693f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset...')\n",
    "def encode_json(filename):\n",
    "    result_chunks = []\n",
    "    for context, question, answer_start, answer_end, question_id, answer in zip(*read_squad_json(filename)):\n",
    "        if len(question) > 0 and '？' not in question:\n",
    "            question = question.replace('?', '？')\n",
    "            if '？' not in question:\n",
    "                question = question + '？'\n",
    "        enc_context, ctx_posisions = enc.encode(context, clean=False, position=True)\n",
    "        enc_question = enc.encode(question, clean=False, position=False)\n",
    "        token_start = -1 if answer_start<0 else np.argmax(np.array(ctx_posisions+[1000000]) >= answer_start)\n",
    "        token_end = 0 if answer_end<=0 else np.argmax(np.array(ctx_posisions+[1000000]) >= answer_end)\n",
    "        ctx_offset = 1 + len(enc_question) + 2\n",
    "        tokens = [CLS_TOKEN] + enc_question + [SEP_TOKEN, CLS_TOKEN] + enc_context + [EOT_TOKEN]\n",
    "        tokens_weights = [1.0] * len(tokens)\n",
    "        token_start = min(len(tokens)-2, token_start + ctx_offset)\n",
    "        token_end = max(token_start, token_end + ctx_offset - 1)\n",
    "        while len(tokens) < max_seq_length:\n",
    "            tokens.append(EOT_TOKEN)\n",
    "            tokens_weights.append(0.0)\n",
    "        tokens = tokens[:max_seq_length]\n",
    "        tokens_weights = tokens_weights[:max_seq_length]\n",
    "        if token_start >= max_seq_length:\n",
    "            token_start = ctx_offset-1\n",
    "            token_end = ctx_offset-1\n",
    "        elif token_end >= max_seq_length:\n",
    "            token_end = max_seq_length-1\n",
    "        answer = context[answer_start:answer_end]\n",
    "        result_chunks.append({\"tokens\":tokens,\"tokens_weights\":tokens_weights,\"token_start\":token_start,\"token_end\":token_end,\"question\":question,\n",
    "                              \"ctx_offset\":ctx_offset,\"ctx_posisions\":ctx_posisions,\"context\":context,\"answer\":answer,\"question_id\":question_id})\n",
    "    return result_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d0077b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 2 contexts from summalize-testdata.json.\n"
     ]
    }
   ],
   "source": [
    "global_chunks = encode_json(dataset) #if do_training else None\n",
    "global_chunk_index = 0\n",
    "global_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cbe0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(fn, chunks):\n",
    "    data = []\n",
    "    for preds in run_predict(chunks):\n",
    "        answers = []\n",
    "        context = preds[\"context\"]\n",
    "        question = preds[\"question\"]\n",
    "        for pred, pred_pos in zip(preds[\"predictionstrings\"],preds[\"predictionpositions\"]):\n",
    "            if len(pred) > 0:\n",
    "                answers.append({\"text\":pred,\"answer_start\":pred_pos})\n",
    "        qas = {\"id\":preds[\"question_id\"],\"question\":question,\"is_impossible\":preds[\"impossible\"],\"answers\":answers}\n",
    "        data.append({\"paragraphs\":[{\"context\":context,\"qas\":[qas]}]})\n",
    "    with open(fn, \"w\", encoding=\"utf-8\") as wf:\n",
    "        wf.write(json.dumps({\"data\":data}, ensure_ascii=False , indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0097649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(input_chunks):\n",
    "            tokens,tokens_weights,ctx_offset,ctx_posisions,context,question_id,answer,question = [], [], [], [], [], [], [], []\n",
    "            pp=[]\n",
    "            for chunk in input_chunks:\n",
    "                tokens.append(chunk[\"tokens\"])\n",
    "                tokens_weights.append(chunk[\"tokens_weights\"])\n",
    "                ctx_offset.append(chunk[\"ctx_offset\"])\n",
    "                ctx_posisions.append(chunk[\"ctx_posisions\"])\n",
    "                context.append(chunk[\"context\"])\n",
    "                question_id.append(chunk[\"question_id\"])\n",
    "                answer.append(chunk[\"answer\"])\n",
    "                question.append(chunk[\"question\"])\n",
    "                pp.append(\"true_y: %d %d\"%(chunk[\"token_start\"],chunk[\"token_end\"]))\n",
    "            tokens = np.array(tokens, dtype=np.int32)\n",
    "            tokens_weights = np.array(tokens_weights, dtype=np.float32)\n",
    "            pred = lossmodel.predict([tokens,tokens_weights], batch_size=batch_size)\n",
    "            result = []\n",
    "            pi = 0\n",
    "            for starts, ends, off, pos, ctx, qid, ans, qes in zip(pred[0], pred[1], ctx_offset, ctx_posisions, context, question_id, answer, question):\n",
    "                selected = []\n",
    "                impossible = False\n",
    "                p_starts = get_best_indexes(starts, num_best_indexes)\n",
    "                p_ends = get_best_indexes(ends, num_best_indexes)\n",
    "                pi += 1\n",
    "                for start_index in p_starts:\n",
    "                    for end_index in p_ends:\n",
    "                        if start_index==off-1 and end_index==off-1 and len(selected)==0:\n",
    "                            impossible = True\n",
    "                        if start_index-off >= len(pos) or start_index<off:\n",
    "                            continue\n",
    "                        if end_index-off >= len(pos) or end_index<off:\n",
    "                            continue\n",
    "                        if end_index < start_index:\n",
    "                            continue\n",
    "                        length = end_index - start_index + 1\n",
    "                        if length > max_answer_length:\n",
    "                            continue\n",
    "                        selected.append((start_index, end_index))\n",
    "                predictionstrings = []\n",
    "                predictionpositions = []\n",
    "                for p_start,p_end in selected:\n",
    "                    start_token = p_start-off\n",
    "                    end_token = p_end-off\n",
    "                    start_pos = pos[start_token]\n",
    "                    end_pos = pos[end_token+1] if end_token+1<len(pos) else len(ctx)\n",
    "                    predictionstrings.append(ctx[start_pos:end_pos])\n",
    "                    predictionpositions.append(start_pos)\n",
    "                result.append({\"predictionstrings\":predictionstrings, \"predictionpositions\":predictionpositions,\n",
    "                               \"impossible\":impossible, \"answer\":ans, \"question_id\":qid, \"context\":ctx, \"question\":qes})\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7defba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 2 contexts from summalize-testdata.json.\n"
     ]
    }
   ],
   "source": [
    "do_prediction = len(pred_dataset) > 0 and os.path.isfile(pred_dataset)\n",
    "prediction_chunks = encode_json(pred_dataset) if do_prediction else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b38d0b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 270 contexts from squad-predicted.json.\n"
     ]
    }
   ],
   "source": [
    "pred('squad-predicted.json', prediction_chunks)\n",
    "result = encode_json('squad-predicted.json')\n",
    "question_id = np.array([res[\"question_id\"] for res in result])\n",
    "question = [res[\"question\"] for res in result]\n",
    "answer = [res[\"answer\"] for res in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbd7e49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\tAnswer\n",
      "[Context]\n",
      "東京株式市場において日経平均株価が値上がりし、3万670円10銭の値で終えた。株高の背景には新型コロナウイルス感染拡大の終息と景気回復への期待感があり、今後は企業業績の回復が焦点になる。日経平均株価が3万円の大台を回復するのは約30年半ぶり。関係者には過熱感を警戒する見方もあり、しばらくは国内外の感染状況を見ながらの取り引きが続きそう。トピックスも21円16銭値上がりし、2118円87銭で終える。出来高は13億3901万株。\n",
      "[Answer]\n",
      "日経平均株価が3万円の大台を回復するのは約30年半ぶり\n",
      "[Context]\n",
      "リーガ・エスパニョーラのレガネスはバジャドリードと対戦。23分、エリア内でバジャドリードのハンドによりPKを獲得するも惜しくも外れる。その後の30分にはオスカル・ロドリゲスが先制点を挙げる。1点ビハインドのバジャドリードは49分、エネス・ウナルがゴールを決めるがオフサイドの判定でゴールは取り消された。試合はそのままレガネスが1対0で逃げ切る。\n",
      "[Answer]\n",
      "試合はそのままレガネスが1対0で逃げ切る\n"
     ]
    }
   ],
   "source": [
    "index = np.arange(len(result))\n",
    "\n",
    "print('Question\\tAnswer')\n",
    "for qid in np.unique(question_id):\n",
    "    i = sorted(index[np.where(question_id == qid)])[0]\n",
    "\n",
    "    print(\"[Context]\")\n",
    "    print(result[i][\"context\"])\n",
    "    if len(question[i]) > 0:\n",
    "        print(\"[Question]\")\n",
    "        print(question[i])\n",
    "    print(\"[Answer]\")\n",
    "    print(answer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35004f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
